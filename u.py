# -*- coding: utf-8 -*-
"""U.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQ_QKx-VjdgUd-J17a2Mpqs2y6XyBP8a
"""

!pip install torch transformers gradio

# edututorai.py
import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_NAME = "ibm-granite/granite-3.2-2b-instruct"

# Detect device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Load model with appropriate dtype/device handling
if torch.cuda.is_available():
    # On GPU, use float16 and automatic device map (if supported)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True
    )
else:
    # On CPU: keep float32 and load to CPU
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float32,
        device_map=None,
        low_cpu_mem_usage=True
    )
    model.to(device)

# Ensure pad token exists
if tokenizer.pad_token is None:
    # Set pad token to eos token (some models don't have a pad token)
    tokenizer.pad_token = tokenizer.eos_token

# Helper: generate text
def generate_response(prompt: str, max_new_tokens: int = 256, temperature: float = 0.7) -> str:
    """
    Generate a response for the given prompt.
    Uses max_new_tokens to avoid truncating the prompt.
    """
    if not prompt:
        return "Please provide a prompt."

    # Tokenize (truncation keeps the prompt within model context window)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)

    # Move tensors to correct device if needed
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # Generate. Use max_new_tokens to append to the prompt.
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            top_p=0.95,
            top_k=50,
            num_return_sequences=1
        )

    # Decode and remove the original prompt from the returned text (if repeated)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # If model echoes the prompt, strip it
    if decoded.startswith(prompt):
        decoded = decoded[len(prompt):].strip()
    return decoded.strip()

# Specific UI functions
def concept_explanation(concept: str) -> str:
    prompt = (
        f"Explain the concept of '{concept}' in detail suitable for an intermediate student. "
        "Include a clear definition, step-by-step explanation, and at least two examples with short, worked solutions."
    )
    return generate_response(prompt, max_new_tokens=500, temperature=0.6)

def quiz_generator(concept: str) -> str:
    prompt = (
        f"Generate 5 quiz questions about '{concept}' with different question types "
        "(multiple choice, true/false, short answer). Number them. At the end, provide a separate 'ANSWERS' section "
        "with concise answers."
    )
    return generate_response(prompt, max_new_tokens=600, temperature=0.7)

# Build Gradio interface
with gr.Blocks() as app:
    gr.Markdown("# ðŸ“š EduTutor AI â€” Educational Assistant")

    with gr.Tabs():
        with gr.TabItem("Concept Explanation"):
            gr.Markdown("Enter a concept (e.g., `machine learning`, `binary search`, `photosynthesis`) and get a detailed explanation.")
            concept_input = gr.Textbox(label="Enter a concept", placeholder="e.g., machine learning")
            explain_btn = gr.Button("Explain")
            explanation_output = gr.Textbox(label="Explanation", lines=15)
            explain_btn.click(fn=concept_explanation, inputs=concept_input, outputs=explanation_output)

        with gr.TabItem("Quiz Generator"):
            gr.Markdown("Enter a topic and generate a 5-question quiz with an answers section.")
            quiz_input = gr.Textbox(label="Enter a topic", placeholder="e.g., physics")
            quiz_btn = gr.Button("Generate Quiz")
            quiz_output = gr.Textbox(label="Quiz Questions", lines=18)
            quiz_btn.click(fn=quiz_generator, inputs=quiz_input, outputs=quiz_output)

    gr.Markdown("### Notes\n- The model may sometimes produce slightly incorrect facts; always verify answers for critical use.\n- If you run on CPU the model may be slow â€” using a GPU is recommended for faster response times.")

if __name__ == "__main__":
    # share=True creates a public link (useful for quick testing). Remove for local-only serving.
    app.launch(share=True)